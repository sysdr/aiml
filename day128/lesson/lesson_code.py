"""
Day 128: Activation Functions â€” The Decision Gates of Neural Networks
180-Day AI/ML Course | Module 4: Deep Learning

Implements ReLU, Sigmoid, Tanh, Softmax from scratch with forward + backward passes.
Gradient check validates correctness against PyTorch autograd.
"""

import numpy as np
import argparse


# â”€â”€ Base Class â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class Activation:
    """Base activation function. All subclasses implement forward() and backward()."""

    def forward(self, x: np.ndarray) -> np.ndarray:
        raise NotImplementedError

    def backward(self, grad: np.ndarray) -> np.ndarray:
        """
        Receives upstream gradient (dL/d_output).
        Returns downstream gradient (dL/d_input) via chain rule.
        """
        raise NotImplementedError

    def __call__(self, x: np.ndarray) -> np.ndarray:
        return self.forward(x)


# â”€â”€ ReLU â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ReLU(Activation):
    """
    Rectified Linear Unit: f(x) = max(0, x)
    Derivative: 1 if x > 0, else 0

    Use: Hidden layers of deep networks (CNNs, transformers, MLPs)
    Strength: Fast, sparse, avoids vanishing gradient
    Weakness: Dead neurons (x stuck < 0)
    """

    def forward(self, x: np.ndarray) -> np.ndarray:
        self.cache = x  # store input for backward pass
        return np.maximum(0, x)

    def backward(self, grad: np.ndarray) -> np.ndarray:
        # Gradient passes through where input was positive, zero otherwise
        return grad * (self.cache > 0).astype(float)


# â”€â”€ Leaky ReLU â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class LeakyReLU(Activation):
    """
    f(x) = x if x > 0, else alpha * x
    Fixes dead neuron problem. Used in Tesla Autopilot, GANs.
    """

    def __init__(self, alpha: float = 0.01):
        self.alpha = alpha

    def forward(self, x: np.ndarray) -> np.ndarray:
        self.cache = x
        return np.where(x > 0, x, self.alpha * x)

    def backward(self, grad: np.ndarray) -> np.ndarray:
        dx = np.where(self.cache > 0, 1.0, self.alpha)
        return grad * dx


# â”€â”€ Sigmoid â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class Sigmoid(Activation):
    """
    f(x) = 1 / (1 + exp(-x))  â†’  output âˆˆ (0, 1)
    Derivative: f(x) * (1 - f(x))

    Use: Binary classification output, LSTM gates
    Strength: Probabilistic interpretation
    Weakness: Saturates â†’ vanishing gradients in hidden layers
    """

    def forward(self, x: np.ndarray) -> np.ndarray:
        # Numerically stable: clamp before exp to avoid overflow
        x_clipped = np.clip(x, -500, 500)
        self.output = 1.0 / (1.0 + np.exp(-x_clipped))
        return self.output

    def backward(self, grad: np.ndarray) -> np.ndarray:
        # d/dx sigmoid(x) = sigmoid(x) * (1 - sigmoid(x))
        return grad * self.output * (1.0 - self.output)


# â”€â”€ Tanh â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class Tanh(Activation):
    """
    f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))  â†’  output âˆˆ (-1, 1)
    Derivative: 1 - tanhÂ²(x)

    Use: LSTM/GRU cell state, hidden layers when zero-centering matters
    Strength: Zero-centered (better gradient flow than sigmoid)
    Weakness: Still saturates at extremes
    """

    def forward(self, x: np.ndarray) -> np.ndarray:
        self.output = np.tanh(x)
        return self.output

    def backward(self, grad: np.ndarray) -> np.ndarray:
        return grad * (1.0 - self.output ** 2)


# â”€â”€ Softmax â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class Softmax(Activation):
    """
    f(xáµ¢) = exp(xáµ¢) / Î£â±¼ exp(xâ±¼)  â†’  outputs sum to 1.0
    
    Use: Multiclass classification output, language model token prediction
    Note: In practice, fused with CrossEntropyLoss for numerical stability.
          Standalone backward shown here for educational clarity.
    """

    def forward(self, x: np.ndarray) -> np.ndarray:
        # Subtract max for numerical stability (prevents exp overflow)
        x_shifted = x - np.max(x, axis=-1, keepdims=True)
        exp_x = np.exp(x_shifted)
        self.output = exp_x / np.sum(exp_x, axis=-1, keepdims=True)
        return self.output

    def backward(self, grad: np.ndarray) -> np.ndarray:
        """
        Full Jacobian-vector product for softmax.
        For 1D input: dL/dxáµ¢ = sáµ¢(Î´áµ¢â±¼ - sâ±¼) Â· grad_j summed over j
        """
        s = self.output
        # Vectorized Jacobian: diag(s) - s @ s.T for each sample
        if s.ndim == 1:
            jacobian = np.diag(s) - np.outer(s, s)
            return jacobian @ grad
        else:
            # Batched version
            batch_size, n = s.shape
            result = np.zeros_like(grad)
            for i in range(batch_size):
                si = s[i]
                jacobian = np.diag(si) - np.outer(si, si)
                result[i] = jacobian @ grad[i]
            return result


# â”€â”€ Activation Registry â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ACTIVATIONS = {
    'relu': ReLU,
    'leaky_relu': LeakyReLU,
    'sigmoid': Sigmoid,
    'tanh': Tanh,
    'softmax': Softmax,
}


def get_activation(name: str, **kwargs) -> Activation:
    """Factory function used by MLP builder (Day 129)."""
    name = name.lower()
    if name not in ACTIVATIONS:
        raise ValueError(f"Unknown activation '{name}'. Choose from: {list(ACTIVATIONS.keys())}")
    return ACTIVATIONS[name](**kwargs)


# â”€â”€ Gradient Check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def numerical_gradient(fn, x: np.ndarray, eps: float = 1e-5) -> np.ndarray:
    """
    Numerically estimate gradient via finite differences.
    Used to verify analytic backward() implementations.
    This is exactly how production teams validate custom CUDA kernels.
    """
    grad = np.zeros_like(x)
    it = np.nditer(x, flags=['multi_index'])
    while not it.finished:
        idx = it.multi_index
        x_plus = x.copy();  x_plus[idx] += eps
        x_minus = x.copy(); x_minus[idx] -= eps
        # Sum output to get scalar for differentiation
        grad[idx] = (fn(x_plus).sum() - fn(x_minus).sum()) / (2 * eps)
        it.iternext()
    return grad


def gradient_check(activation: Activation, x: np.ndarray, tol: float = 1e-5) -> bool:
    """Compare analytic gradient to numerical gradient."""
    # Forward pass
    activation.forward(x)
    # Analytic gradient (ones upstream gradient â†’ dL/dx where L = sum(output))
    analytic = activation.backward(np.ones_like(x))
    # Numerical gradient
    numeric = numerical_gradient(activation.forward, x)
    
    rel_error = np.abs(analytic - numeric) / (np.abs(analytic) + np.abs(numeric) + 1e-8)
    max_err = rel_error.max()
    passed = max_err < tol
    return passed, max_err


# â”€â”€ Demo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def run_demo():
    """Visualize all activation functions and their derivatives."""
    try:
        import matplotlib.pyplot as plt
    except ImportError:
        print("matplotlib not installed. Run: pip install matplotlib")
        return

    x = np.linspace(-5, 5, 300)

    activations_to_plot = [
        ('ReLU',    ReLU(),    'tab:blue'),
        ('Sigmoid', Sigmoid(), 'tab:orange'),
        ('Tanh',    Tanh(),    'tab:green'),
    ]

    fig, axes = plt.subplots(2, 3, figsize=(14, 8))
    fig.suptitle('Day 128: Activation Functions â€” Forward & Derivative', fontsize=14, fontweight='bold')

    for col, (name, act, color) in enumerate(activations_to_plot):
        y = act.forward(x)
        dy = act.backward(np.ones_like(x))

        ax_top = axes[0, col]
        ax_top.plot(x, y, color=color, linewidth=2.5)
        ax_top.axhline(0, color='gray', linewidth=0.5, linestyle='--')
        ax_top.axvline(0, color='gray', linewidth=0.5, linestyle='--')
        ax_top.set_title(f'{name} â€” f(x)', fontweight='bold')
        ax_top.set_ylim(-1.5, 1.5)
        ax_top.grid(True, alpha=0.3)

        ax_bot = axes[1, col]
        ax_bot.plot(x, dy, color=color, linewidth=2.5, linestyle='--')
        ax_bot.axhline(0, color='gray', linewidth=0.5, linestyle='--')
        ax_bot.set_title(f"{name} â€” f'(x)", fontweight='bold')
        ax_bot.set_ylim(-0.5, 1.5)
        ax_bot.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig('activation_functions.png', dpi=120, bbox_inches='tight')
    print("ğŸ“Š Plot saved: activation_functions.png")
    plt.show()

    # XOR demo â€” show non-linear separation is possible with activations
    print("\nğŸ”¬ XOR Dataset (2 features, 4 points):")
    xor_x = np.array([[0,0],[0,1],[1,0],[1,1]], dtype=float)
    xor_y = np.array([0, 1, 1, 0])

    relu = ReLU()
    # Single hidden layer manual pass (weights chosen to solve XOR)
    W1 = np.array([[1, 1],[-1,-1]], dtype=float).T   # 2x2
    b1 = np.array([-0.5, 1.5])
    W2 = np.array([1, -1], dtype=float)
    b2 = np.array([0.0])

    h = relu.forward(xor_x @ W1 + b1)
    out = 1 / (1 + np.exp(-(h @ W2 + b2)))  # sigmoid output
    pred = (out > 0.5).astype(int)

    for i, (xi, yi, pi) in enumerate(zip(xor_x, xor_y, pred)):
        status = "âœ“" if yi == pi else "âœ—"
        print(f"  Input {xi.astype(int)} â†’ Target: {yi} | Predicted: {pi} {status}")


# â”€â”€ Entry Point â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Day 128: Activation Functions')
    parser.add_argument('--demo', action='store_true', help='Run visualization demo')
    parser.add_argument('--check', action='store_true', help='Run gradient checks')
    args = parser.parse_args()

    if args.demo:
        run_demo()

    elif args.check:
        print("ğŸ”¬ Running gradient checks...\n")
        np.random.seed(42)
        checks = [
            ('ReLU',      ReLU(),      np.random.randn(4, 3)),
            ('LeakyReLU', LeakyReLU(), np.random.randn(4, 3)),
            ('Sigmoid',   Sigmoid(),   np.random.randn(4, 3)),
            ('Tanh',      Tanh(),      np.random.randn(4, 3)),
        ]
        for name, act, x in checks:
            passed, err = gradient_check(act, x)
            status = "âœ… PASSED" if passed else "âŒ FAILED"
            print(f"  {name:12s} gradient check: {status}  (max rel error: {err:.2e})")

    else:
        print("Day 128: Activation Functions")
        print("  --demo   Visualize all activations + XOR demo")
        print("  --check  Run gradient checks vs numerical gradients")
